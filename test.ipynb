{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A10, compute capability 8.6, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A10) - 22268 MiB free\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 579 tensors from openthai14b-quantized.model (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = wip14B\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 15B\n",
      "llama_model_loader: - kv   4:                          qwen2.block_count u32              = 48\n",
      "llama_model_loader: - kv   5:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   6:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   7:                  qwen2.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   8:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   9:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  23:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:  241 tensors\n",
      "llama_model_loader: - type q4_K:  289 tensors\n",
      "llama_model_loader: - type q6_K:   49 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 8.37 GiB (4.87 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 5120\n",
      "print_info: n_layer          = 48\n",
      "print_info: n_head           = 40\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 5\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 13824\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = -1\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 14B\n",
      "print_info: model params     = 14.77 B\n",
      "print_info: general.name     = wip14B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 152064\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  37 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  38 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  39 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  40 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  41 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  42 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  43 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  44 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  45 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  46 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  47 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  48 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 48 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 49/49 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  8148.38 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   417.66 MiB\n",
      "..........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:  CUDA_Host  output buffer size =     0.58 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: kv_size = 512, type_k = 'f16', type_v = 'f16', n_layer = 48, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  32: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  33: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  34: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  35: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  36: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  37: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  38: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  39: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  40: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  41: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  42: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  43: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  44: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  45: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  46: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  47: dev = CUDA0\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =    96.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:      CUDA0 compute buffer size =   307.00 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    11.01 MiB\n",
      "llama_context: graph nodes  = 1782\n",
      "llama_context: graph splits = 2\n",
      "CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.embedding_length': '5120', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'wip14B', 'qwen2.block_count': '48', 'general.type': 'model', 'general.size_label': '15B', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '8', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.attention.head_count': '40', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.quantization_version': '2', 'qwen2.feed_forward_length': '13824', 'tokenizer.ggml.model': 'gpt2'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "model = Llama(model_path=\"openthai14b-quantized.model\", n_gpu_layers = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     230.10 ms\n",
      "llama_perf_context_print: prompt eval time =     229.94 ms /    34 tokens (    6.76 ms per token,   147.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =     446.80 ms /    20 runs   (   22.34 ms per token,    44.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1106.05 ms /    54 tokens\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "class UserDetail(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "res = model.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"message\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"message\"],\n",
    "        },\n",
    "    },\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-ee1044eb-3c3c-459d-9eb3-176c41751a44',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1747870257,\n",
       " 'model': 'openthai14b-quantized.model',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '{\"message\": \"The Los Angeles Dodgers won the World Series in 2020.\"}'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 34, 'completion_tokens': 20, 'total_tokens': 54}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.xlsum import XLSumDataset\n",
    "ds = XLSumDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('จงสรุปข้อความด้านล่าง\\nข้อความ: เจ้าหน้าที่ทางการระบุว่า ตั้งแต่วันที่ 4 ถึง 15 พ.ย. รถยนต์ที่มีเลขทะเบียนคู่หรือคี่ ต้องสลับวันกันวิ่งบนท้องถนน อินเดียเคยใช้มาตรการดังกล่าวมาแล้ว แต่ไม่มีความชัดเจนว่า ช่วยลดปัญหามลพิษได้จริง ขณะนี้ ในกรุงนิวเดลี ระดับ PM2.5 หรือ อนุภาคในอากาศที่เป็นอันตราย ที่สามารถเข้าไปในปอดได้ สูงเกินระดับปลอดภัย 10 เท่า อย่างไรก็ตาม ผู้เชี่ยวชาญระบุว่า รถยนต์ไม่น่าจะเป็นสาเหตุหลักของมลพิษทางอากาศในกรุงนิวเดลี แต่มาจากการที่เกษตรกรในรัฐข้างเคียง เผาตอซังพืชผลทางการเกษตรหลังเก็บเกี่ยวแล้ว เพื่อเตรียมพื้นที่สำหรับการเพาะปลูกครั้งใหม่ เจ้าหน้าที่สาธารณสุขได้ขอให้ประชาชนอาศัยอยู่แต่ภายในอาคาร และงดเว้นกิจกรรมที่ต้องใช้กำลัง ขณะที่ผู้คนหลายล้านคนเสี่ยงที่จะเจ็บป่วยเกี่ยวกับระบบทางเดินหายใจ โรงเรียนหลายแห่งได้ปิดการเรียนการสอนจนถึงวันอังคาร และคาดว่า จะขยายเวลาปิดไปจนถึงวันศุกร์ ในช่วงที่กรุงนิวเดลีกำลังถูกปกคลุมด้วยหมอกควัน เดินทางแบบไหนได้รับมลพิษมากที่สุด? อาร์วินด์ เคจิริวัล มุขมนตรีของกรุงนิวเดลี ระบุว่า มาตรการสลับวันวิ่ง หรือที่รู้จักกันในชื่อ \"มาตรการวันคู่วันคี่\" จะช่วยลดจำนวนรถยนต์บนท้องถนนลงได้หลายแสนคัน ผู้ที่เพิกเฉยต่อมาตรการดังกล่าว จะถูกปรับ 4,000 รูปี (ประมาณ 1,680 บาท) คิดเป็น 2 เท่าของค่าปรับเดิม ทางการยกเว้นการบังคับใช้กฎนี้กับรถขนส่งสาธารณะ รถฉุกเฉิน แท็กซี่ และรถประเภท 2 ล้อ ส่วนผู้หญิงที่ขับรถเพียงลำพัง จะได้รับการยกเว้นด้วย อะไรเป็นสาเหตุของมลพิษ ผู้เชี่ยวชาญระบุว่า มลพิษที่มาจากยานพาหนะต่าง ๆ เป็นเพียงหนึ่งในหลายปัจจัยที่ทำให้กรุงนิวเดลี กลายเป็น \"ห้องรมควัน\" ตามคำที่นายเคจิริวัลใช้ สาเหตุสำคัญที่ทำให้ระดับมลพิษสูงในช่วงเวลานี้ของปีคือ เกษตรกรในรัฐข้างเคียงเผาตอซังพืชผลทางการเกษตร เพื่อแผ้วถางพื้นที่เตรียมการเพาะปลูกรอบใหม่ การทำเช่นนั้น ทำให้เกิดอนุภาคขนาดเล็กที่เป็นอันตรายร้ายแรงหลายชนิด ทั้ง คาร์บอนไดออกไซด์ ไนโตรเจนไดออกไซด์ และซัลเฟอร์ไดออกไซด์ ขณะที่การจุดดอกไม้ไฟในช่วงเทศกาลดิวาลี (Diwali) เมื่อหนึ่งสัปดาห์ก่อน ได้ทำให้ระดับอนุภาคเหล่านี้เพิ่มสูงขึ้นไปอีก นอกจากนี้ มลพิษจากอุตสาหกรรมและการก่อสร้างก็มีส่วนทำให้เกิดหมอกควันเช่นกัน ความพยายามในการระบุสาเหตุของปัญหา ทำให้เกิดความขัดแย้งขึ้นระหว่างนักการเมืองระดับชาติและระดับรัฐ โดยนายเคจิริวัล เรียกร้องให้รัฐปัญจาบและรัฐหรยาณา ที่อยู่ข้างเคียงปราบปรามการจุดไฟเผาเพื่อเตรียมพื้นที่การเกษตร เรื่องนี้ทำให้นายประกาช จาวัดเอคาร์ รัฐมนตรีสิ่งแวดล้อมของอินเดีย กล่าวหาว่า นายเคจิริวัล ทำเรื่องนี้ให้เป็นการเมือง ด้วยทำให้รัฐข้างเคียงกลายเป็น \"ผู้ร้าย\" ขณะที่ชาวอินเดียทั่วไปกำลังหวังว่า ฝนที่ตกกระจายในสัปดาห์หน้า จะช่วยชำระล้างมลพิษให้หมดไป โดยคาดว่า ฝนจะตกลงมาในวันพฤหัสบดีนี้ ปัญหาหมอกควันเลวร้ายแค่ไหน ไม่มีทางหลีกเลี่ยงหมอกควันที่ปกคลุมทั่วเมืองได้ ซิดดาร์ท ซิงห์ นักวิจัยด้านนโยบายสภาพภูมิอากาศ และผู้เขียนหนังสือเรื่อง The Great Smog of India ระบุว่า อากาศในกรุงนิวเดลี อบอวลไปด้วย \"กลิ่นคล้ายใบไม้ไหม้\" \"ทุกคนรู้สึกได้ มันเต็มไปด้วยหมอกควัน ทำให้เคืองตา และเจ็บคอ\" เขากล่าวกับบีบีซี ช่วงหนึ่งของวัน ระดับของ PM2.5 ในเมืองหลวงของอินเดีย เพิ่มสูงกว่าระดับ PM2.5 ในกรุงปักกิ่งของจีนถึง 7 เท่า โดยกรุงปักกิ่งก็พยายามต่อสู้กับปัญหามลพิษที่คล้ายคลึงกันในช่วงหลายปีที่ผ่านมา เจ้าหน้าที่กระทรวงสาธารณสุขของอินเดีย ระบุว่า การเฝ้าระวังมลพิษในกรุงนิวเดลี ไม่มีความละเอียดพอที่จะบันทึกระดับมลพิษได้อย่างแม่นยำ ซึ่งเขาเรียกปัญหานี้ว่า \"หายนะ\" เมื่อวันศุกร์ที่ผ่านมา มีการแจกหน้ากาก 5 ล้านชิ้นตามโรงเรียนต่าง ๆ หลังจากที่ทางการประกาศภาวะฉุกเฉินด้านสาธารณสุข ทำไมปัญหานี้จึงเลวร้ายมากในช่วงไม่กี่ปีที่ผ่านมา ส่วนหนึ่งเป็นเพราะการเปลี่ยนแปลงของวัฏจักรการเพาะปลูกและการเก็บเกี่ยวพืชผลทางการเกษตรในรัฐปัญจาบและรัฐหรยาณา ซึ่งเป็นรัฐที่ทำการเกษตรเป็นหลัก 10 ปีก่อน 2 รัฐนี้ได้ผ่านกฎหมายที่ออกมาเพื่ออนุรักษ์น้ำบาดาล ซึ่งส่งผลให้เกษตรกรจำต้องปลูกข้าวในช่วงกลางเดือน มิ.ย. แทนที่จะเป็นสิ้นเดือน เม.ย. ตามที่เคยปลูก เพื่อให้เกษตรกรได้ใช้ประโยชน์จากน้ำฝนที่ตกลงมาในฤดูมรสุมในการปลูกพืชที่ต้องการน้ำมาก วัฏจักรการเพาะปลูกที่เลื่อนออกไป ทำให้วัฏจักรการเก็บเกี่ยวผลผลิตเลื่อนตามไปด้วย เกษตรกรมีเวลาในเตรียมพื้นที่เพาะปลูกรอบใหม่น้อยลงมาก และการเผาตอซังพืชก็เป็นวิธีที่ได้ผลและประหยัดที่สุดในการเตรียมพื้นที่เพาะปลูก เคราะห์ร้าย ปัญหานี้เกิดขึ้นประจวบกับรูปแบบการเคลื่อนตัวของลมที่เปลี่ยนแปลงไปในกรุงนิวเดลี และพื้นที่ทางตอนเหนือของอินเดีย ผลการศึกษาของมหาวิทยาลัยคอร์เนลล์ ซึ่งตีพิมพ์เมื่อเดือน ก.ค. ระบุว่า \"เงื่อนไขที่ลงตัวกันอย่างเหมาะเจาะในช่วงเดือนพ.ย. ทำให้อนุภาคขนาดเล็กในอากาศมากระจุกตัวรวมกันเพิ่มสูงขึ้นเกือบ 30%\" ในด้านภูมิศาสตร์ กรุงนิวเดลีไม่มีทางออกสู่ทะเลและตั้งอยู่ในพื้นที่ราบที่ถูกโอบล้อมด้วยเทือกเขาหิมาลัย นั่นหมายความว่า ทำให้ผลกระทบยิ่งรุนแรงเพิ่มขึ้นไปอีก ขณะที่การจราจรในเมืองใหญ่ก็มีส่วนในการทำให้เกิดปัญหานี้ขึ้นเช่นกัน มาตรการสลับวันรถวิ่งทำอย่างไร ทีมงานหลายร้อยทีมทั้งจากตำรวจ หน่วยงานขนส่ง และอาสาสมัครพลเมือง จะถูกส่งไปตรวจตราการใช้มาตรการนี้ ซึ่งเคยบังคับใช้มาแล้วในปี 2016 และ 2017 เชื่อกันว่า มาตรการนี้จะทำให้ระบบขนส่งสาธารณะมีผู้มาใช้บริการหนาแน่นขึ้น แต่เจ้าหน้าที่ระบุว่า จะเพิ่มจำนวนการให้บริการ ระเบียบใหม่นี้ กำหนดให้ รถที่มีทะเบียนลงท้ายด้วยเลขคี่ คือ 1, 3, 5, 7 หรือ 9 สามารถวิ่งบนถนนได้ในวันคี่ คือ 5, 7, 9, 11, 13 และ 15 พ.ย ส่วนรถที่มีทะเบียนลงท้ายด้วยเลข 0, 2, 4, 6 หรือ 8 จะขับรถได้ในวันคู่ คือ 4, 6, 8, 12 และ 14 พ.ย. มาตรการนี้บังคับใช้ระหว่างเวลา 08.00-20.00 น. ตั้งแต่วันศุกร์ถึงวันเสาร์ และจะบังคับใช้กับรถยนต์ที่มาจากนอกเมืองหลวงด้วย ส่วนวันอาทิตย์ไม่บังคับใช้กฎนี้ ส่วนที่ต่างจากปีก่อน ๆ คือ ปีนี้ ยานพาหนะที่ใช้พลังงานที่เป็นมิตรต่อสิ่งแวดล้อมมากกว่า เช่น CNG (ไทยเรียก NGV) จะต้องทำตามกฎนี้ด้วย ขณะที่รถยนต์ไฟฟ้ายังคงได้รับการยกเว้น มุขมนตรีกรุงนิวเดลี และมุขมนตรีรัฐอื่น ๆ ก็ต้องทำตามกฎนี้โดยไม่ได้รับการยกเว้นเช่นกัน แม้ว่าจะมีผู้ที่ได้รับการยกเว้นจำนวนมาก รวมถึง ประธานาธิบดีและนายกรัฐมนตรี, เจ้าหน้าที่การทูตจากต่างประเทศ, ผู้หญิงที่ขับรถเพียงลำพัง หรือรถที่มีแต่ผู้หญิงนั่ง และรถที่กำลังเดินทางไปโรงพยาบาล ถ้าพวกเขาสามารถยืนยันได้ว่ามีเหตุฉุกเฉิน\\nสรุป:', 'กรุงนิวเดลี เมืองหลวงของอินเดีย ออกมาตรการสลับวันขับรถยนต์ตามเลขทะเบียน เพื่อรับมือกับระดับมลพิษที่เพิ่มสูงจนเป็นอันตราย')\n"
     ]
    }
   ],
   "source": [
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
